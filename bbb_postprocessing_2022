#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun  1 09:20:30 2022

@author: kepecs
"""


import tifffile as tif, matplotlib.pyplot as plt, numpy as np, os, pandas as pd, json, copy
from scipy.stats import ttest_ind as ttest
from statsmodels.stats.multitest import multipletests

#postprocessing
fl = "/home/kepecs/Documents/cadaverine_slices/modified_ch01/csv_data"
ontology_file = "/home/kepecs/Documents/allen.json"
csvs = [os.path.join(fl, xx) for xx in os.listdir(fl) if "csv" in xx and "AP" not in xx]
#preprocessing to make into dataframe like amy's

def get_progeny(dic,parent_structure,progeny_list):
   
    if "msg" in list(dic.keys()): dic = dic["msg"][0]
    
    name = dic.get("name")
    children = dic.get("children")
    if name == parent_structure:
        for child in children: # child is a dict
            child_name = child.get("name")
            progeny_list.append(child_name)
            get_progeny(child,parent_structure=child_name,progeny_list=progeny_list)
        return
    
    for child in children:
        child_name = child.get("name")
        get_progeny(child,parent_structure=parent_structure,progeny_list=progeny_list)
    return 

with open(ontology_file) as json_file:
    ontology_dict = json.load(json_file)
    
#get voxel counts from brainpipe
df = pd.DataFrame()
allen = pd.read_excel("/home/kepecs/Documents/allen_id_table_w_voxel_counts.xlsx")
df["name"] = allen.name
df["voxels_in_structure"] = allen["voxels_in_structure"]

from allensdk.api.queries.ontologies_api import OntologiesApi
oapi = OntologiesApi()
structure_graph = oapi.get_structures_with_sets([1])  # 1 is the id of the adult mouse structure graph
from allensdk.core.structure_tree import StructureTree

# This removes some unused fields returned by the query
structure_graph = StructureTree.clean_structures(structure_graph)  
tree = StructureTree(structure_graph)
# get the ids of all the structure sets in the tree
structure_set_ids = tree.get_structure_sets()

# query the API for information on those structure sets
allen_stuff = pd.DataFrame(oapi.get_structure_sets(structure_set_ids))
target_group = 167587189 #summary structures
summary_structures = tree.get_structures_by_set_id([target_group])
#%%
#get names of summary structures
sum_structs = [xx for xx in pd.DataFrame(summary_structures)["name"].values if xx != "fiber tracts"]
#count voxels in regions
vox = pd.DataFrame()
vox["name"] = sum_structs
for area in sum_structs: 
    progeny = []; get_progeny(ontology_dict, area, progeny)
    try: 
        voxs = [allen.loc[allen.name == area, "voxels_in_structure"].values]
    except:
        voxs = []
    for prog in progeny:
        try:
            voxs.append(allen.loc[allen.name == prog, "voxels_in_structure"].values)
        except:
            voxs.append(0)
    vox.loc[vox.name == area, "voxels_in_structure"] = np.sum(voxs)

#compile # of times structure shows up in a csv        
#compile csvs
nmdf = pd.DataFrame(); nmdf["name"] = sum_structs
#also make a dict to store AP coords for striatum!
for csv in csvs:
    nm = os.path.basename(csv)[:13]
    csv_ = pd.read_csv(csv)
    csv_soi = csv_["name"].value_counts() # cell counts
    #add AP axis to df and export
    apdf = pd.DataFrame(csv_soi)
    if len(csv_soi) > 0:
        apdf["AP"] = [np.unique(csv_.AP)[0]]*len(csv_soi); apdf.columns = ["id", "AP"]
        apdf.to_csv(csv[:-4]+"_w_counts_AP.csv")
    for soi in sum_structs:
        progeny = []; get_progeny(ontology_dict, soi, progeny)
        #try except statements are to make sure all sois and children are included
        try:
            #if struct has counts by itself
            counts = [csv_soi[soi]]
            nmdf.loc[nmdf.name == soi, nm]=1
        except:
            counts = []
            #init the animal column in the struct count df
            nmdf.loc[nmdf.name == soi, nm]=0
        for prog in progeny:
            try: 
                counts.append(csv_soi[prog])
            except:
                counts.append(0)
        try: #if an entry already exists
            df.loc[df.name == soi, nm] = df.loc[df.name == soi, nm].sum() + np.sum(counts)
            nmdf.loc[nmdf.name == soi, nm] = nmdf.loc[nmdf.name == soi, nm]+len(progeny)
            nmdf.loc[nmdf.name == soi, nm+"_AP"] = nmdf.loc[nmdf.name == soi, nm+"_AP"].append(np.unique(csv_.AP)[0])
        except:
            df.loc[df.name == soi, nm] = np.sum(counts)
            nmdf.loc[nmdf.name == soi, nm] = len(progeny)
            
nmdf.to_csv('/home/kepecs/Documents/cadaverine_slices/no_of_times_structures_quantified_per_animal.csv')

#Calculate % counts
#order of animals
animals = ['cadw2_rhrh_ca', 'cadw1_lh_cach', 'pr2w2_lhrh_ca', 'cadw2_lh_cach', 
     'pr2w2_rh_ctrl', 'pr2w2_rhrh_ct', 'cadw2_nh_ctrl']
for nm in df.columns[2:]: #only for animal columns
    total = np.nansum(df[nm])
    df[nm+"_percent_count"] = [xx/total if xx != np.nan else np.nan for xx in df[nm]] #avoid nans
#%%            
#filter out regions with all nans
dfp = df.dropna(how = "all", subset = animals)
dfp.index = dfp.name
dfp = dfp.drop(columns = "name")
#add voxel column
for area in dfp.index:
    if vox.loc[vox.name == area, "voxels_in_structure"].values>0:
        dfp.loc[dfp.index == area, "voxels_in_structure"] = vox.loc[vox.name == area, "voxels_in_structure"].values

dfd = dfp.copy()
#get density
for animal in animals:
    dfd[animal] = dfp[animal]/dfp["voxels_in_structure"]
dfd = dfd.drop(columns = "voxels_in_structure").dropna(how = "all", subset = animals)[animals]   
#%%
#shuffle regions
#reorgnize df
dfp = df.dropna(how = "all", subset = animals) #remove areas with NaNs
#only get structures that are present in all cachexia animals
ca_animals = ['cadw2_rhrh_ca', 'cadw1_lh_cach', 'pr2w2_lhrh_ca', 'cadw2_lh_cach']
dfp.index = dfp.name
dfp = dfp.drop(columns = "name")
#add voxel column
for area in dfp.index:
    if vox.loc[vox.name == area, "voxels_in_structure"].values>0:
        dfp.loc[dfp.index == area, "voxels_in_structure"] = vox.loc[vox.name == area, "voxels_in_structure"].values
#only get regions which dont have all zeros in the dataframe where i quantified occurence of structure
structs_quantified = nmdf[~nmdf[['cadw2_rhrh_ca', 'cadw1_lh_cach', 'pr2w2_lhrh_ca', 'cadw2_lh_cach']].eq(0).sum(1).ge(3)].name.values
dfp = dfp[dfp.index.isin(structs_quantified)]

ans = ['cadw2_rhrh_ca_percent_count', 'pr2w2_lhrh_ca_percent_count', 'cadw1_lh_cach_percent_count',
       'cadw2_lh_cach_percent_count']
#only include animal % counts
arr = np.array(dfp[ans])
shufs = [arr[np.random.choice(np.arange(len(arr)), replace=False, size=len(arr)),:] for i in range(10000)]
shufmean = np.mean(shufs, axis=0)
#one sided ttest
# dfd["pvalue"]= [ttest(arr[i], shufmean[i])[1] if ttest(arr[i], shufmean[i])[0]>0 else 1 for i in range(len(arr))]
#two sided  ttest
dfp["pvalue"]= [ttest(arr[i], shufmean[i])[1] for i in range(len(arr))]
dfp["qvalue"] = multipletests(dfp.pvalue.values, method="fdr_bh")[1]
dfp.to_csv("/home/kepecs/Documents/cadaverine_slices/all_counts.csv")

#plot BLA, tail of striatum???
#%%
#filter by significance
dfsig = dfp[(dfp.qvalue < 0.1)]# dfsig = dfpp[dfpp.qvalue < 0.1]
#keep only structures with no more than 3 zeros across cachexia animals
#aka atleast 1 cachexia animal has a representation of the area
# structs_to_keep = nmdf[~nmdf[['cadw2_rhrh_ca', 'cadw1_lh_cach', 'pr2w2_lhrh_ca', 'cadw2_lh_cach']].eq(0).sum(1).ge(3)] 
# dfsig[dfsig.index.isin(structs_to_keep.name.values)]
#import ventricle stats
dfv = pd.read_csv("/home/kepecs/Documents/cadaverine_slices/distance_from_ventricle.csv", index_col = None)
#import vasculature stats
dfvasc = pd.read_csv("/home/kepecs/Documents/cadaverine_slices/vasculature.csv", index_col=None)
#add ventricle distance (min)
#normalize vasculature point size by area
for xx in dfsig.index:
    dfsig.loc[dfsig.index == xx, "median_euclidean_dist_from_ventricle_borders"] = dfv.loc[dfv.name == xx, "max_euclidean_dist_from_ventricle_borders"].values
    #vasculature normalized by area
    try:
        dfsig.loc[dfsig.index == xx, "vasculature_points_norm"] = int(dfvasc.loc[dfvasc["name"] == xx,
        "vasculature_points"])/int(dfsig.loc[dfsig.index == xx, "voxels_in_structure"])
    except Exception as e:
        print(xx, e)
#sort either by euclidean dist or vasculature
dfsig = dfsig.sort_values(by = "median_euclidean_dist_from_ventricle_borders") #sort
#formatting df
dfplot = dfsig.drop(columns = ["voxels_in_structure", 'cadw2_rhrh_ca', 'pr2w2_lhrh_ca', 
                               'cadw1_lh_cach', 'cadw2_lh_cach', 'pr2w2_rh_ctrl', 'pr2w2_rhrh_ct',
       'cadw2_nh_ctrl', 'pvalue', 'qvalue', 'cadw2_nh_ctrl_percent_count', 'pr2w2_rhrh_ct_percent_count', 
       'pr2w2_rh_ctrl_percent_count', 'median_euclidean_dist_from_ventricle_borders','vasculature_points_norm'])
#% counts/voxels in structure!
for an in ans:
    dfplot[an] = dfsig[an]/dfsig["voxels_in_structure"]

#%%    
import seaborn as sns

plt.figure(figsize=(18,0.8))
cmap = copy.copy(plt.cm.Blues)#plt.cm.Reds)
cmap.set_over(plt.cm.Blues(1.0)) #cmap.set_over('maroon')
cmap.set_under('w')
## WORK IN PROGRESS
p = sns.heatmap(dfplot.T, xticklabels = dfplot.index, 
                yticklabels = ['cadw2_rhrh', 'pr2w2_lhrh', 'cadw1_lh', 'cadw2_lh'],
                cmap = cmap, 
                # norm = LogNorm(),
                vmin=0, vmax=1.3e-7, 
                cbar_kws={'label': '% + cells/total voxels'})
#how to quantify density?
p.set_xticklabels(dfplot.index, size = 7)
p.set_xlabel("Regions, min to max distance from a brain border or ventricle")
plt.savefig("/home/kepecs/Desktop/vent_norm.pdf", bbox_inches = "tight")

#%%
# split plots by regions and AP coordinates of striatum
src = "/home/kepecs/Documents/cadaverine_slices/modified_ch01/csv_data"
dfs = [os.path.join(src, xx) for xx in os.listdir(src) if "AP" in xx]
for i, an in enumerate(ca_animals):
    dfs_ca = [xx for xx in dfs if an in xx]
    if i == 0:
        df_ca = pd.concat([pd.read_csv(xx) for xx in dfs_ca])
        df_ca["animal"] = [an]*len(df_ca)
    else:
        df_ca_ = pd.concat([pd.read_csv(xx) for xx in dfs_ca])
        df_ca_["animal"] = [an]*len(df_ca_)
        df_ca = df_ca.merge(df_ca_, how = "outer")
    
#define tail of striatum
tail_ap = -1.2
#arrange df_ca columns
df_ca.columns = ['name', 'count', 'AP', 'animal']
all_striatum = df_ca[(df_ca.name == "Caudoputamen")]
all_striatum["tail"] = all_striatum.AP < tail_ap

sns.factorplot(x = 'animal', y='count', 
               hue = 'tail',data=all_striatum, kind='bar')
sns.stripplot(x="animal", y="count", hue="tail",
                   data=all_striatum, dodge=True, color = ".3")
plt.savefig("/home/kepecs/Desktop/str_counts.pdf", bbox_inches = "tight")
