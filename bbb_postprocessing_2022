#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun  1 09:20:30 2022

@author: kepecs
"""

import tifffile as tif, matplotlib.pyplot as plt, numpy as np, os, pandas as pd, json, copy
from scipy.stats import ttest_ind as ttest
from statsmodels.stats.multitest import multipletests

#postprocessing
fl = "/home/kepecs/Documents/cadaverine_slices/modified_ch01/csv_data"
ontology_file = "/home/kepecs/Documents/allen.json"
csvs = [os.path.join(fl, xx) for xx in os.listdir(fl) if "csv" in xx and "AP" not in xx]
#preprocessing to make into dataframe like amy's

def get_progeny(dic,parent_structure,progeny_list):
   
    if "msg" in list(dic.keys()): dic = dic["msg"][0]
    
    name = dic.get("name")
    children = dic.get("children")
    if name == parent_structure:
        for child in children: # child is a dict
            child_name = child.get("name")
            progeny_list.append(child_name)
            get_progeny(child,parent_structure=child_name,progeny_list=progeny_list)
        return
    
    for child in children:
        child_name = child.get("name")
        get_progeny(child,parent_structure=parent_structure,progeny_list=progeny_list)
    return 

with open(ontology_file) as json_file:
    ontology_dict = json.load(json_file)
    
#get voxel counts from brainpipe
allen = pd.read_excel("/home/kepecs/Documents/allen_id_table_w_voxel_counts.xlsx")
from allensdk.api.queries.ontologies_api import OntologiesApi
oapi = OntologiesApi()
structure_graph = oapi.get_structures_with_sets([1])  # 1 is the id of the adult mouse structure graph
from allensdk.core.structure_tree import StructureTree

# This removes some unused fields returned by the query
structure_graph = StructureTree.clean_structures(structure_graph)  
tree = StructureTree(structure_graph)
# get the ids of all the structure sets in the tree
structure_set_ids = tree.get_structure_sets()

# query the API for information on those structure sets
allen_stuff = pd.DataFrame(oapi.get_structure_sets(structure_set_ids))
target_group = 167587189 #summary structures
summary_structures = tree.get_structures_by_set_id([target_group])

#%%%
#get names of summary structures
sum_structs = [xx for xx in pd.DataFrame(summary_structures)["name"].values if xx != "fiber tracts"]
ca_animals = ['cadw2_rhrh_ca', 'cadw1_lh_cach', 'pr2w2_lhrh_ca', 'cadw2_lh_cach']

df = pd.DataFrame()
df["name"] = sum_structs
df["id"]= [xx for xx in pd.DataFrame(summary_structures)["id"].values if xx != 1009]
#count voxels in regions
vox = pd.DataFrame()
vox["name"] = sum_structs
for area in sum_structs: 
    progeny = []; get_progeny(ontology_dict, area, progeny)
    try: 
        voxs = [allen.loc[allen.name == area, "voxels_in_structure"].values]
    except:
        voxs = []
    for prog in progeny:
        try:
            voxs.append(allen.loc[allen.name == prog, "voxels_in_structure"].values)
        except:
            voxs.append(0)
    vox.loc[vox.name == area, "voxels_in_structure"] = np.sum(voxs)

#compile # of times structure shows up in a csv        
#also make a dict to store AP coords for striatum!
for csv in csvs:
    nm = os.path.basename(csv)[:13]
    csv_ = pd.read_csv(csv)
    csv_soi = csv_["name"].value_counts() # cell counts
    #add AP axis to df and export
    apdf = pd.DataFrame(csv_soi)
    if len(csv_soi) > 0:
        apdf["AP"] = [np.unique(csv_.AP)[0]]*len(csv_soi); apdf.columns = ["count", "AP"]
        apdf.to_csv(csv[:-4]+"_w_counts_AP.csv")
    for soi in sum_structs:
        progeny = []; get_progeny(ontology_dict, soi, progeny)
        #try except statements are to make sure all sois and children are included
        try:
            #if struct has counts by itself
            counts = [csv_soi[soi]]
        except:
            counts = []
            #init the animal column in the struct count df
        for prog in progeny:
            try: 
                counts.append(csv_soi[prog])
            except:
                counts.append(0)
        try: #if an entry already exists
            df.loc[df.name == soi, nm] = df.loc[df.name == soi, nm].sum() + np.sum(counts)
        except:
            df.loc[df.name == soi, nm] = np.sum(counts)

###########IMPORTANT###########
#only get regions which dont have all zeros in the dataframe where i quantified occurence of structure
src = "/home/kepecs/Documents/cadaverine_slices/modified_ch01/csv_data"
dfs = [os.path.join(src, xx) for xx in os.listdir(src) if "AP" in xx]
for i, an in enumerate(ca_animals):
    dfs_ca = [xx for xx in dfs if an in xx]
    if i == 0:
        df_ca = pd.concat([pd.read_csv(xx) for xx in dfs_ca])
        df_ca["animal"] = [an]*len(df_ca)
    else:
        df_ca_ = pd.concat([pd.read_csv(xx) for xx in dfs_ca])
        df_ca_["animal"] = [an]*len(df_ca_)
        df_ca = df_ca.merge(df_ca_, how = "outer")
#rename columns
df_ca.columns = ['name', 'count', 'AP', 'animal']
       
#i want to check whether each structure, its parent, or its parent's parent is within a structure on sum_structs
#....
include = []
for soi in sum_structs:
    progeny = [soi]; get_progeny(ontology_dict, soi, progeny)
    incl = [xx for xx in progeny if xx in df_ca.name.unique()]
    if len(incl)>=1:
        include.append((soi))
#filter df by only structures who are /whose parents are quantified
df = df[df.name.isin(include)]
#Calculate % counts
#order of animals
animals = ['cadw2_rhrh_ca', 'cadw1_lh_cach', 'pr2w2_lhrh_ca', 'cadw2_lh_cach', 
     'pr2w2_rh_ctrl', 'pr2w2_rhrh_ct', 'cadw2_nh_ctrl']
for nm in animals: #only for animal columns
    total = np.nansum(df[nm])
    df[nm+"_percent_count"] = [xx/total if xx != np.nan else np.nan for xx in df[nm]] #avoid nans

#%%            
#shuffle regions
#reorgnize df
dfp = df.dropna(how = "all", subset = animals) #remove areas with NaNs
dfp.index = dfp.name
dfp = dfp.drop(columns = "name")
#add voxel column
for area in dfp.index:
    if vox.loc[vox.name == area, "voxels_in_structure"].values>0:
        dfp.loc[dfp.index == area, "voxels_in_structure"] = vox.loc[vox.name == area, "voxels_in_structure"].values

ans = ['cadw2_rhrh_ca_percent_count', 'cadw1_lh_cach_percent_count',
       'pr2w2_lhrh_ca_percent_count', 'cadw2_lh_cach_percent_count']
#only include animal % counts
arr = np.array(dfp[ans])
shufs = [arr[np.random.choice(np.arange(len(arr)), replace=False, size=len(arr)),:] for i in range(100000)]
shufmean = np.mean(shufs, axis=0)
#one sided ttest
# dfd["pvalue"]= [ttest(arr[i], shufmean[i])[1] if ttest(arr[i], shufmean[i])[0]>0 else 1 for i in range(len(arr))]
#two sided  ttest
dfp["pvalue"]= [ttest(arr[i], shufmean[i])[1] for i in range(len(arr))]
dfp["qvalue"] = multipletests(dfp.pvalue.values, method="fdr_bh")[1]
dfp.to_csv("/home/kepecs/Documents/cadaverine_slices/all_counts.csv")

#plot BLA, tail of striatum???
#%%
#filter by significance
dfsig = dfp[(dfp.qvalue < 0.1)]# dfsig = dfpp[dfpp.qvalue < 0.1]
#import ventricle stats
dfv = pd.read_csv("/home/kepecs/Documents/cadaverine_slices/distance_from_ventricle.csv", index_col = None)
#import vasculature stats
dfvasc = pd.read_csv("/home/kepecs/Documents/cadaverine_slices/vasculature.csv", index_col=None)
#add ventricle distance (min)
#normalize vasculature point size by area
for xx in dfsig.index:
    dfsig.loc[dfsig.index == xx, "median_euclidean_dist_from_ventricle_borders"] = dfv.loc[dfv.name == xx, "max_euclidean_dist_from_ventricle_borders"].values
    #vasculature normalized by area
    try:
        dfsig.loc[dfsig.index == xx, "vasculature_points_norm"] = int(dfvasc.loc[dfvasc["name"] == xx,
        "vasculature_points"])/int(dfsig.loc[dfsig.index == xx, "voxels_in_structure"])
    except Exception as e:
        print(xx, e)
#sort either by euclidean dist or vasculature
dfsig = dfsig.sort_values(by = "median_euclidean_dist_from_ventricle_borders") #sort
#formatting df
dfplot = dfsig.drop(columns = ["id","voxels_in_structure", 'cadw2_rhrh_ca', 'pr2w2_lhrh_ca', 
                               'cadw1_lh_cach', 'cadw2_lh_cach', 'pr2w2_rh_ctrl', 'pr2w2_rhrh_ct',
       'cadw2_nh_ctrl', 'pvalue', 'qvalue', 'cadw2_nh_ctrl_percent_count', 'pr2w2_rhrh_ct_percent_count', 
       'pr2w2_rh_ctrl_percent_count', 'median_euclidean_dist_from_ventricle_borders','vasculature_points_norm'])
#% counts/voxels in structure! for plotting
for an in ans:
    dfplot[an] = dfsig[an]/dfsig["voxels_in_structure"]

#%%    
import seaborn as sns

plt.figure(figsize=(20,0.8))
cmap = copy.copy(plt.cm.Blues)#plt.cm.Reds)
cmap.set_over(plt.cm.Blues(1.0)) #cmap.set_over('maroon')
cmap.set_under('w')
## WORK IN PROGRESS
p = sns.heatmap(dfplot.T, xticklabels = dfplot.index, 
                yticklabels = ['cadw2_rhrh', 'pr2w2_lhrh', 'cadw1_lh', 'cadw2_lh'],
                cmap = cmap, 
                # norm = LogNorm(),
                vmin=0, vmax=1.3e-7, 
                cbar_kws={'label': '% + cells/total voxels'})
#how to quantify density?
p.set_xticklabels(dfplot.index, size = 7)
p.set_xlabel("Regions, min to max distance from a brain border or ventricle")
plt.savefig("/home/kepecs/Desktop/vent_norm.pdf", bbox_inches = "tight")

#%%
# split plots by regions and AP coordinates of striatum    
#define tail of striatum
tail_ap = -1.2
all_striatum = df_ca[(df_ca.name == "Caudoputamen")]
all_striatum["tail"] = all_striatum.AP < tail_ap

sns.factorplot(x = 'animal', y='count', 
               hue = 'tail',data=all_striatum, kind='bar')
sns.stripplot(x="animal", y="count", hue="tail",
                   data=all_striatum, dodge=True, color = ".3")
plt.savefig("/home/kepecs/Desktop/str_counts.pdf", bbox_inches = "tight")
#%%

regions = ["Basolateral amygdalar nucleus", "Basomedial amygdalar nucleus", "Lateral amygdalar nucleus",
           "Claustrum", "Posterior amygdalar nucleus", "Caudoputamen", "Nucleus accumbens", "Bed nuclei of the stria terminalis",
           "Paraventricular nucleus of the thalamus", "Arcuate hypothalamic nucleus", "Visual areas",
           "Auditory areas", "Anterior cingulate area, dorsal part", "Anterior cingulate area, ventral part",
           "Prelimbic area", "Infralimbic area", "Orbital area, lateral part", "Orbital area, medial part", "Orbital area, ventrolateral part",
           "Primary motor area", "Gustatory areas", "Visceral area", "Supplemental somatosensory area"]
allreg = dfp[(dfp.index.isin(regions))]
all_animals = ['cadw2_rhrh_ca', 'cadw1_lh_cach', 'pr2w2_lhrh_ca', 'cadw2_lh_cach', 
               'pr2w2_rh_ctrl', 'pr2w2_rhrh_ct', 'cadw2_nh_ctrl']
colors = ["g", "g", "g", "g", "k", "k", "k"]
markers = ["*", "v", "o", "s", "*", "v", "s"]
for an in all_animals:
    for xx in regions:
        allreg.loc[allreg.index == xx, an+"_density"] = allreg.loc[allreg.index == xx, an]/(allreg.loc[allreg.index == xx, "voxels_in_structure"]* (1/25)**3)

for i,an in enumerate(all_animals):
    g = sns.scatterplot(x = 'name', y=an+"_density", 
                   data=allreg, color = colors[i], marker = markers[i])
#log scale
g.set_yscale("log")
# Put the legend out of the figure
plt.legend(all_animals, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xticks(rotation = 90)
plt.ylabel("density (cells/mm$^3$)")
plt.savefig("/home/kepecs/Desktop/reg_counts_log.pdf", bbox_inches = "tight")
plt.savefig("/home/kepecs/Desktop/reg_counts_log.jpg", bbox_inches = "tight")
