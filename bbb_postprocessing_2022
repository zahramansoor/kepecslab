#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun  1 09:20:30 2022

@author: kepecs
"""


import tifffile as tif, matplotlib.pyplot as plt, numpy as np, os, pandas as pd, json, copy
from scipy.stats import ttest_ind as ttest
from statsmodels.stats.multitest import multipletests

#postprocessing
fl = "/home/kepecs/Documents/cadaverine_slices/modified_ch01/csv_data"
ontology_file = "/home/kepecs/Documents/allen.json"
csvs = [os.path.join(fl, xx) for xx in os.listdir(fl) if "csv" in xx]
#preprocessing to make into dataframe like amy's

def get_progeny(dic,parent_structure,progeny_list):
   
    if "msg" in list(dic.keys()): dic = dic["msg"][0]
    
    name = dic.get("name")
    children = dic.get("children")
    if name == parent_structure:
        for child in children: # child is a dict
            child_name = child.get("name")
            progeny_list.append(child_name)
            get_progeny(child,parent_structure=child_name,progeny_list=progeny_list)
        return
    
    for child in children:
        child_name = child.get("name")
        get_progeny(child,parent_structure=parent_structure,progeny_list=progeny_list)
    return 

with open(ontology_file) as json_file:
    ontology_dict = json.load(json_file)
    
#get voxel counts from brainpipe
df = pd.DataFrame()
allen = pd.read_excel("/home/kepecs/Documents/allen_id_table_w_voxel_counts.xlsx")
df["name"] = allen.name
df["voxels_in_structure"] = allen["voxels_in_structure"]

from allensdk.api.queries.ontologies_api import OntologiesApi
oapi = OntologiesApi()
structure_graph = oapi.get_structures_with_sets([1])  # 1 is the id of the adult mouse structure graph
from allensdk.core.structure_tree import StructureTree

# This removes some unused fields returned by the query
structure_graph = StructureTree.clean_structures(structure_graph)  
tree = StructureTree(structure_graph)
# get the ids of all the structure sets in the tree
structure_set_ids = tree.get_structure_sets()

# query the API for information on those structure sets
allen_stuff = pd.DataFrame(oapi.get_structure_sets(structure_set_ids))
target_group = 167587189 #summary structures
summary_structures = tree.get_structures_by_set_id([target_group])
#%%
sum_structs = pd.DataFrame(summary_structures)["name"]
#count voxels in regions
vox = pd.DataFrame()
vox["name"] = sum_structs
for area in sum_structs: 
    progeny = []; get_progeny(ontology_dict, area, progeny)
    try: 
        voxs = [allen.loc[allen.name == area, "voxels_in_structure"].values]
    except:
        voxs = []
    for prog in progeny:
        try:
            voxs.append(allen.loc[allen.name == prog, "voxels_in_structure"].values)
        except:
            voxs.append(0)
    vox.loc[vox.name == area, "voxels_in_structure"] = np.sum(voxs)
        
#compile csvs
for csv in csvs:
    nm = os.path.basename(csv)[:13]
    csv_ = pd.read_csv(csv)
    csv_soi = csv_["name"].value_counts() # cell counts
    # csv_soi = csv_["name"].value_counts()/len(csv_) #% cell counts
    for soi in sum_structs:
        progeny = []; get_progeny(ontology_dict, soi, progeny)
        #try except statements are to make sure all sois and children are included
        try:
            counts = [csv_soi[soi]]             
        except:
            counts = []
        for prog in progeny:
            try: 
                counts.append(csv_soi[prog])
            except:
                counts.append(0)
        try: #if an entry already exists
            df.loc[df.name == soi, nm] = df.loc[df.name == soi, nm].sum() + np.sum(counts)
        except:
            df.loc[df.name == soi, nm] = np.sum(counts)
#Calculate % counts
animals = ['cadw2_rhrh_ca', 'pr2w2_lhrh_ca', 'cadw1_lh_cach', 'cadw2_lh_cach',
           'pr2w2_rh_ctrl', 'pr2w2_rhrh_ct', 'cadw2_nh_ctrl']

for nm in animals:
    total = np.nansum(df[nm])
    df[nm+"_percent_count"] = [xx/total if xx != np.nan else np.nan for xx in df[nm]] #avoid nans
#%%            
#filter out nan
import seaborn as sns
from matplotlib.colors import LogNorm

dfp = df.dropna(how = "all", subset = animals)
dfp.index = dfp.name
dfp = dfp.drop(columns = "name")
#add voxel column
for area in dfp.index:
    if vox.loc[vox.name == area, "voxels_in_structure"].values>0:
        dfp.loc[dfp.index == area, "voxels_in_structure"] = vox.loc[vox.name == area, "voxels_in_structure"].values

dfd = dfp.copy()
#get density
for animal in animals:
    dfd[animal] = dfp[animal]/dfp["voxels_in_structure"]
dfd = dfd.drop(columns = "voxels_in_structure").dropna(how = "all", subset = animals)[animals]   
#%%
#shuffle regions
#reorgnize df
dfp = df.dropna(how = "all", subset = animals)
dfp.index = dfp.name
dfp = dfp.drop(columns = "name")
#add voxel column
for area in dfp.index:
    if vox.loc[vox.name == area, "voxels_in_structure"].values>0:
        dfp.loc[dfp.index == area, "voxels_in_structure"] = vox.loc[vox.name == area, "voxels_in_structure"].values


ans = ['cadw2_rhrh_ca_percent_count', 'pr2w2_lhrh_ca_percent_count', 'cadw1_lh_cach_percent_count',
       'cadw2_lh_cach_percent_count']
arr = np.array(dfp[ans])
shufs = [arr[np.random.choice(np.arange(len(arr)), replace=False, size=len(arr)),:] for i in range(10000)]
shufmean = np.mean(shufs, axis=0)
#one sided ttest
# dfd["pvalue"]= [ttest(arr[i], shufmean[i])[1] if ttest(arr[i], shufmean[i])[0]>0 else 1 for i in range(len(arr))]
#two sided  ttest
dfp["pvalue"]= [ttest(arr[i], shufmean[i])[1] for i in range(len(arr))]
dfp["qvalue"] = multipletests(dfp.pvalue.values, method="fdr_bh")[1]
#only get regions which dont have all zeros
dfpp = dfp[(dfp[ans[0]]!=0) | (dfp[ans[1]]!=0) | (dfp[ans[2]]!=0)]
dfpp.to_csv("/home/kepecs/Desktop/text.csv")
#filter by significance
dfsig = dfpp[dfpp.qvalue < 0.1]
#import ventricle stats
dfv = pd.read_csv("/home/kepecs/Documents/cadaverine_slices/distance_from_ventricle.csv", index_col = None)
#import vasculature stats
dfvasc = pd.read_csv("/home/kepecs/Documents/cadaverine_slices/vasculature.csv", index_col=None)
#add ventricle distance (min)
#normalize vasculature point size by area
for xx in dfsig.index:
    dfsig.loc[dfsig.index == xx, "median_euclidean_dist_from_ventricle_borders"] = dfv.loc[dfv.name == xx, "median_euclidean_dist_from_ventricle_borders"].values
    #vasculature normalized by area
    dfsig.loc[dfsig.index == xx, "vasculature_points_norm"] = int(dfvasc.loc[dfvasc["name"] == xx,
    "vasculature_points"])/int(dfsig.loc[dfsig.index == xx, "voxels_in_structure"])

#sort either by euclidean dist or vasculature
dfsig = dfsig.sort_values(by = "median_euclidean_dist_from_ventricle_borders") #sort
#formatting df
dfplot = dfsig.drop(columns = ["voxels_in_structure", 'cadw2_rhrh_ca', 'pr2w2_lhrh_ca', 
                               'cadw1_lh_cach', 'cadw2_lh_cach', 'pr2w2_rh_ctrl', 'pr2w2_rhrh_ct',
       'cadw2_nh_ctrl', 'pvalue', 'qvalue', 'cadw2_nh_ctrl_percent_count', 'pr2w2_rhrh_ct_percent_count', 
       'pr2w2_rh_ctrl_percent_count', 'median_euclidean_dist_from_ventricle_borders','vasculature_points_norm'])
for an in ans:
    dfplot[an] = dfsig[an]/dfsig["voxels_in_structure"]

#%%    
plt.figure(figsize=(15,0.8))
cmap = copy.copy(plt.cm.Blues)#plt.cm.Reds)
cmap.set_over(plt.cm.Blues(1.0)) #cmap.set_over('maroon')
cmap.set_under('w')
## WORK IN PROGRESS
p = sns.heatmap(dfplot.T, xticklabels = dfplot.index, 
                yticklabels = ['cadw2_rhrh', 'pr2w2_lhrh', 'cadw1_lh', 'cadw2_lh'],
                cmap = cmap, 
                # norm = LogNorm(),
                vmin=0, vmax=2e-7, 
                cbar_kws={'label': '% + cells/total voxels'})
#how to quantify density?
p.set_xticklabels(dfplot.index, size = 7)
p.set_xlabel("Regions, min to max distance from a brain border or ventricle")
plt.savefig("/home/kepecs/Desktop/vent_norm.pdf", bbox_inches = "tight")

#%%
#split plots by regions
# nc = ["Caudoputamen", "Nucleus accumbens","Olfactory tubercle", "Lateral septal nucleus", "Pallidum"]
# plt.figure(figsize=(2,5))
# cmap = copy.copy(plt.cm.Blues)#plt.cm.Reds)
# cmap.set_over(plt.cm.Blues(1.0)) #cmap.set_over('maroon')
# cmap.set_under('w')
# p = sns.heatmap(dfp[dfd.index.isin(nc)].drop(columns = "voxels_in_structure"), xticklabels = animals, cmap = cmap, 
#                 # norm = LogNorm(),
#                 vmin=0, vmax=200, 
#                 cbar_kws={'label': 'cadaverine + cells'})
# #how to quantify density?
# p.set_yticklabels(p.get_yticklabels(), size = 8)

# plt.savefig("/home/kepecs/Desktop/str_counts.jpg", bbox_inches = "tight")
